{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethiopian Sign Language - TWO HAND Landmark Model V2\n",
    "## Improved version with better accuracy!\n",
    "\n",
    "**Improvements over V1:**\n",
    "- Uses BOTH hands (126 features = 2 hands x 21 landmarks x 3 coords)\n",
    "- Better data augmentation (rotation, scaling, noise)\n",
    "- Deeper neural network with residual connections\n",
    "- Class weighting for imbalanced data\n",
    "- More robust to lighting and position variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 1: INSTALL & SETUP ============\n",
    "!pip uninstall numpy -y\n",
    "!pip install numpy==1.26.4 -q\n",
    "!pip install mediapipe -q\n",
    "\n",
    "print('Installation complete!')\n",
    "print('>>> RESTART RUNTIME NOW: Runtime > Restart session')\n",
    "print('>>> Then run from STEP 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 2: MOUNT DRIVE & SET PATHS ============\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ===== CHANGE THESE PATHS TO YOUR FOLDER! =====\n",
    "DATA_DIR = '/content/drive/MyDrive/eth_frames/train'\n",
    "MODEL_SAVE_PATH = '/content/drive/MyDrive/eth_landmark_v2.keras'\n",
    "LABELS_SAVE_PATH = '/content/drive/MyDrive/eth_landmark_labels_v2.json'\n",
    "\n",
    "print('Paths configured!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 3: IMPORTS ============\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Add, Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "print(f'TensorFlow: {tf.__version__}')\n",
    "print(f'GPU: {tf.config.list_physical_devices(\"GPU\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 4: CHECK DATASET ============\n",
    "print('Checking dataset...')\n",
    "\n",
    "if os.path.exists(DATA_DIR):\n",
    "    classes = sorted([d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))])\n",
    "    print(f'Found {len(classes)} classes:')\n",
    "    total_images = 0\n",
    "    class_counts = {}\n",
    "    for i, cls in enumerate(classes):\n",
    "        cls_path = os.path.join(DATA_DIR, cls)\n",
    "        images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        total_images += len(images)\n",
    "        class_counts[cls] = len(images)\n",
    "        print(f'  {i}: {cls} - {len(images)} images')\n",
    "    print(f'Total images: {total_images}')\n",
    "else:\n",
    "    print(f'ERROR: Data folder not found: {DATA_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 5: EXTRACT TWO-HAND LANDMARKS ============\n",
    "def extract_two_hand_landmarks(image_path, hands):\n",
    "    \"\"\"\n",
    "    Extract landmarks from BOTH hands.\n",
    "    Returns 126 features (2 hands x 21 landmarks x 3 coords)\n",
    "    If only one hand detected, the other hand is filled with zeros.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "    \n",
    "    # Initialize both hands with zeros\n",
    "    left_hand = np.zeros(63)  # 21 landmarks x 3 coords\n",
    "    right_hand = np.zeros(63)\n",
    "    \n",
    "    if results.multi_hand_landmarks and results.multi_handedness:\n",
    "        for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "            # Get hand type (Left or Right)\n",
    "            hand_type = handedness.classification[0].label\n",
    "            \n",
    "            # Extract landmarks\n",
    "            landmarks = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "            if hand_type == 'Left':\n",
    "                left_hand = np.array(landmarks)\n",
    "            else:\n",
    "                right_hand = np.array(landmarks)\n",
    "    \n",
    "    # Check if at least one hand was detected\n",
    "    if np.sum(np.abs(left_hand)) == 0 and np.sum(np.abs(right_hand)) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Concatenate both hands: [left_hand(63) + right_hand(63)] = 126 features\n",
    "    return np.concatenate([left_hand, right_hand])\n",
    "\n",
    "# Extract landmarks from all images\n",
    "print('Extracting TWO-HAND landmarks from images...')\n",
    "print('This may take several minutes...\\n')\n",
    "\n",
    "X_data = []\n",
    "y_data = []\n",
    "failed_images = []\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=2,  # IMPORTANT: Detect up to 2 hands!\n",
    "    min_detection_confidence=0.5\n",
    ") as hands:\n",
    "    \n",
    "    for class_idx, class_name in enumerate(tqdm(classes, desc='Processing classes')):\n",
    "        class_path = os.path.join(DATA_DIR, class_name)\n",
    "        images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        for img_name in images:\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            landmarks = extract_two_hand_landmarks(img_path, hands)\n",
    "            \n",
    "            if landmarks is not None:\n",
    "                X_data.append(landmarks)\n",
    "                y_data.append(class_idx)\n",
    "            else:\n",
    "                failed_images.append(f'{class_name}/{img_name}')\n",
    "\n",
    "X_data = np.array(X_data)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "print(f'\\nExtraction complete!')\n",
    "print(f'Total samples: {len(X_data)}')\n",
    "print(f'Failed (no hands): {len(failed_images)}')\n",
    "print(f'Feature shape: {X_data.shape} (should be [N, 126])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 6: VISUALIZE SAMPLE ============\n",
    "print('Sample images with detected landmarks:')\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "with mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5) as hands:\n",
    "    sample_idx = 0\n",
    "    for i, class_name in enumerate(classes[:8]):\n",
    "        class_path = os.path.join(DATA_DIR, class_name)\n",
    "        images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        if images:\n",
    "            img_path = os.path.join(class_path, images[0])\n",
    "            image = cv2.imread(img_path)\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(image_rgb)\n",
    "            \n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(image_rgb, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            ax = axes[sample_idx // 4, sample_idx % 4]\n",
    "            ax.imshow(image_rgb)\n",
    "            num_hands = len(results.multi_hand_landmarks) if results.multi_hand_landmarks else 0\n",
    "            ax.set_title(f'{class_name} ({num_hands} hands)')\n",
    "            ax.axis('off')\n",
    "            sample_idx += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 7: PREPARE DATA ============\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42, stratify=y_data\n",
    ")\n",
    "\n",
    "num_classes = len(classes)\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Compute class weights for imbalanced data\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(f'Training samples: {len(X_train)}')\n",
    "print(f'Test samples: {len(X_test)}')\n",
    "print(f'Number of classes: {num_classes}')\n",
    "print(f'Input shape: 126 features (2 hands x 21 landmarks x 3 coords)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 8: ADVANCED DATA AUGMENTATION ============\n",
    "def augment_landmarks_advanced(landmarks, num_augmented=8):\n",
    "    \"\"\"\n",
    "    Advanced augmentation for two-hand landmarks.\n",
    "    \"\"\"\n",
    "    augmented = [landmarks]\n",
    "    \n",
    "    for _ in range(num_augmented):\n",
    "        aug = landmarks.copy()\n",
    "        \n",
    "        # 1. Add random noise\n",
    "        noise = np.random.normal(0, 0.015, aug.shape)\n",
    "        aug_noise = aug + noise\n",
    "        augmented.append(aug_noise)\n",
    "        \n",
    "        # 2. Scale (simulate distance)\n",
    "        scale = np.random.uniform(0.9, 1.1)\n",
    "        aug_scale = aug * scale\n",
    "        augmented.append(aug_scale)\n",
    "        \n",
    "        # 3. Shift position\n",
    "        shift_x = np.random.uniform(-0.1, 0.1)\n",
    "        shift_y = np.random.uniform(-0.1, 0.1)\n",
    "        aug_shift = aug.copy()\n",
    "        # Shift x coordinates (every 3rd starting from 0)\n",
    "        aug_shift[0::3] += shift_x\n",
    "        # Shift y coordinates (every 3rd starting from 1)\n",
    "        aug_shift[1::3] += shift_y\n",
    "        augmented.append(aug_shift)\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "print('Augmenting training data...')\n",
    "X_train_aug = []\n",
    "y_train_aug = []\n",
    "\n",
    "for x, y in tqdm(zip(X_train, y_train), total=len(X_train), desc='Augmenting'):\n",
    "    augmented = augment_landmarks_advanced(x, num_augmented=5)\n",
    "    X_train_aug.extend(augmented)\n",
    "    y_train_aug.extend([y] * len(augmented))\n",
    "\n",
    "X_train_aug = np.array(X_train_aug)\n",
    "y_train_aug = to_categorical(np.array(y_train_aug), num_classes)\n",
    "\n",
    "print(f'Original training samples: {len(X_train)}')\n",
    "print(f'Augmented training samples: {len(X_train_aug)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 9: BUILD IMPROVED MODEL ============\n",
    "def build_landmark_model(input_shape=126, num_classes=20):\n",
    "    \"\"\"\n",
    "    Deeper model with residual connections for better accuracy.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(input_shape,))\n",
    "    \n",
    "    # First block\n",
    "    x = Dense(512, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Second block with residual\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Third block\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Fourth block\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Output\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "model = build_landmark_model(input_shape=126, num_classes=num_classes)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "print(f'\\nTotal parameters: {model.count_params():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 10: TRAIN MODEL ============\n",
    "callbacks = [\n",
    "    ModelCheckpoint(MODEL_SAVE_PATH, monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "    EarlyStopping(monitor='val_accuracy', patience=25, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "print('Training model with class weights...')\n",
    "history = model.fit(\n",
    "    X_train_aug, y_train_aug,\n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 11: PLOT TRAINING HISTORY ============\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "ax1.set_title('Model Accuracy', fontsize=14)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(history.history['loss'], label='Train', linewidth=2)\n",
    "ax2.plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "ax2.set_title('Model Loss', fontsize=14)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 12: EVALUATE MODEL ============\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "best_model = keras.models.load_model(MODEL_SAVE_PATH)\n",
    "test_loss, test_acc = best_model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "\n",
    "print(f'\\nTest Accuracy: {test_acc*100:.2f}%')\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "y_pred = best_model.predict(X_test, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred_classes, target_names=classes))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 13: SAVE LABELS ============\n",
    "labels_dict = {str(i): classes[i] for i in range(len(classes))}\n",
    "\n",
    "with open(LABELS_SAVE_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(labels_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f'Labels saved to: {LABELS_SAVE_PATH}')\n",
    "print(f'\\nLabel mapping:')\n",
    "for idx, label in labels_dict.items():\n",
    "    print(f'  {idx}: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 14: DOWNLOAD FILES ============\n",
    "print('='*60)\n",
    "print('TRAINING COMPLETE!')\n",
    "print('='*60)\n",
    "print(f'\\nModel: {MODEL_SAVE_PATH}')\n",
    "print(f'Labels: {LABELS_SAVE_PATH}')\n",
    "print(f'\\nTest Accuracy: {test_acc*100:.2f}%')\n",
    "print(f'Features: 126 (2 hands x 21 landmarks x 3 coords)')\n",
    "\n",
    "from google.colab import files\n",
    "files.download(MODEL_SAVE_PATH)\n",
    "files.download(LABELS_SAVE_PATH)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 4
}