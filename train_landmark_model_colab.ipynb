{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethiopian Sign Language - MediaPipe Landmark Model\n",
    "## Much better than image-based model!\n",
    "\n",
    "This extracts hand landmarks (21 points x 3 coordinates = 63 features) from your images,\n",
    "then trains a model on landmarks instead of raw pixels.\n",
    "\n",
    "**Benefits:**\n",
    "- Works regardless of background\n",
    "- Works regardless of clothing\n",
    "- Works in different lighting\n",
    "- Much faster prediction\n",
    "- Smaller model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 1: INSTALL & MOUNT ============\n",
    "!pip install mediapipe -q\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ===== CHANGE THESE PATHS! =====\n",
    "DATA_DIR = \"/content/drive/MyDrive/eth_frames/train\"  # Your training images folder\n",
    "MODEL_SAVE_PATH = \"/content/drive/MyDrive/eth_landmark_model.keras\"\n",
    "LABELS_SAVE_PATH = \"/content/drive/MyDrive/eth_landmark_labels.json\"\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 2: IMPORTS ============\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"MediaPipe loaded!\")\n",
    "print(f\"GPU: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 3: CHECK DATASET ============\n",
    "print(\"Checking dataset...\")\n",
    "\n",
    "if os.path.exists(DATA_DIR):\n",
    "    classes = sorted([d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))])\n",
    "    print(f\"\\nâœ… Found {len(classes)} classes:\")\n",
    "    total_images = 0\n",
    "    for i, cls in enumerate(classes):\n",
    "        cls_path = os.path.join(DATA_DIR, cls)\n",
    "        images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        total_images += len(images)\n",
    "        print(f\"  {i}: {cls} - {len(images)} images\")\n",
    "    print(f\"\\nTotal images: {total_images}\")\n",
    "else:\n",
    "    print(f\"âŒ Data folder not found: {DATA_DIR}\")\n",
    "    print(\"Please check your path!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 4: EXTRACT LANDMARKS FROM IMAGES ============\n",
    "def extract_hand_landmarks(image_path, hands):\n",
    "    \"\"\"\n",
    "    Extract hand landmarks from an image.\n",
    "    Returns 63 features (21 landmarks x 3 coordinates) or None if no hand detected.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert BGR to RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process the image\n",
    "    results = hands.process(image_rgb)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        # Get the first hand detected\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        \n",
    "        # Extract 21 landmarks (x, y, z for each)\n",
    "        landmarks = []\n",
    "        for lm in hand_landmarks.landmark:\n",
    "            landmarks.extend([lm.x, lm.y, lm.z])\n",
    "        \n",
    "        return np.array(landmarks)  # 63 features\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Extract landmarks from all images\n",
    "print(\"Extracting hand landmarks from images...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "X_data = []  # Landmarks\n",
    "y_data = []  # Labels\n",
    "failed_images = []\n",
    "\n",
    "# Use MediaPipe Hands\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5\n",
    ") as hands:\n",
    "    \n",
    "    for class_idx, class_name in enumerate(tqdm(classes, desc=\"Processing classes\")):\n",
    "        class_path = os.path.join(DATA_DIR, class_name)\n",
    "        images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        class_success = 0\n",
    "        for img_name in images:\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            landmarks = extract_hand_landmarks(img_path, hands)\n",
    "            \n",
    "            if landmarks is not None:\n",
    "                X_data.append(landmarks)\n",
    "                y_data.append(class_idx)\n",
    "                class_success += 1\n",
    "            else:\n",
    "                failed_images.append(f\"{class_name}/{img_name}\")\n",
    "        \n",
    "        # print(f\"  {class_name}: {class_success}/{len(images)} hands detected\")\n",
    "\n",
    "X_data = np.array(X_data)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "print(f\"\\nâœ… Extraction complete!\")\n",
    "print(f\"Total samples with hands detected: {len(X_data)}\")\n",
    "print(f\"Failed to detect hands: {len(failed_images)}\")\n",
    "print(f\"Feature shape: {X_data.shape}\")\n",
    "\n",
    "if len(failed_images) > 0 and len(failed_images) <= 20:\n",
    "    print(f\"\\nFailed images: {failed_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 5: VISUALIZE SAMPLE LANDMARKS ============\n",
    "def visualize_landmarks(image_path, hands):\n",
    "    \"\"\"Show image with hand landmarks drawn\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image_rgb, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    \n",
    "    return image_rgb\n",
    "\n",
    "# Show sample images with landmarks\n",
    "print(\"Sample images with detected landmarks:\")\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "with mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5) as hands:\n",
    "    sample_idx = 0\n",
    "    for i, class_name in enumerate(classes[:8]):\n",
    "        class_path = os.path.join(DATA_DIR, class_name)\n",
    "        images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        if images:\n",
    "            img_path = os.path.join(class_path, images[0])\n",
    "            img_with_landmarks = visualize_landmarks(img_path, hands)\n",
    "            \n",
    "            ax = axes[sample_idx // 4, sample_idx % 4]\n",
    "            ax.imshow(img_with_landmarks)\n",
    "            ax.set_title(class_name)\n",
    "            ax.axis('off')\n",
    "            sample_idx += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 6: PREPARE DATA ============\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42, stratify=y_data\n",
    ")\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(classes)\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Input shape: {X_train.shape[1]} features (21 landmarks x 3 coordinates)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 7: DATA AUGMENTATION FOR LANDMARKS ============\n",
    "def augment_landmarks(landmarks, num_augmented=5):\n",
    "    \"\"\"\n",
    "    Augment landmark data by adding small random noise.\n",
    "    This simulates slight variations in hand position.\n",
    "    \"\"\"\n",
    "    augmented = [landmarks]\n",
    "    \n",
    "    for _ in range(num_augmented):\n",
    "        # Add small random noise (simulates slight movement)\n",
    "        noise = np.random.normal(0, 0.02, landmarks.shape)\n",
    "        augmented.append(landmarks + noise)\n",
    "        \n",
    "        # Scale slightly (simulates distance from camera)\n",
    "        scale = np.random.uniform(0.95, 1.05)\n",
    "        augmented.append(landmarks * scale)\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "# Augment training data\n",
    "print(\"Augmenting training data...\")\n",
    "X_train_aug = []\n",
    "y_train_aug = []\n",
    "\n",
    "for x, y in zip(X_train, y_train):\n",
    "    augmented = augment_landmarks(x, num_augmented=3)\n",
    "    X_train_aug.extend(augmented)\n",
    "    y_train_aug.extend([y] * len(augmented))\n",
    "\n",
    "X_train_aug = np.array(X_train_aug)\n",
    "y_train_aug = to_categorical(np.array(y_train_aug), num_classes)\n",
    "\n",
    "print(f\"Original training samples: {len(X_train)}\")\n",
    "print(f\"Augmented training samples: {len(X_train_aug)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 8: BUILD MODEL ============\n",
    "print(\"Building landmark model...\")\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(63,)),  # 21 landmarks x 3 coordinates\n",
    "    \n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "print(f\"\\nModel parameters: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 9: TRAIN MODEL ============\n",
    "callbacks = [\n",
    "    ModelCheckpoint(MODEL_SAVE_PATH, monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "    EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"Training model...\")\n",
    "history = model.fit(\n",
    "    X_train_aug, y_train_aug,\n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 10: PLOT TRAINING HISTORY ============\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "ax1.set_title('Model Accuracy', fontsize=14)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(history.history['loss'], label='Train', linewidth=2)\n",
    "ax2.plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "ax2.set_title('Model Loss', fontsize=14)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_SAVE_PATH.replace('.keras', '_history.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 11: EVALUATE MODEL ============\n",
    "print(\"Evaluating model...\")\n",
    "\n",
    "# Load best model\n",
    "best_model = keras.models.load_model(MODEL_SAVE_PATH)\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = best_model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "print(f\"\\nğŸ¯ Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"ğŸ“‰ Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = best_model.predict(X_test, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 12: SAVE LABELS ============\n",
    "# Save class labels\n",
    "labels_dict = {str(i): classes[i] for i in range(len(classes))}\n",
    "\n",
    "with open(LABELS_SAVE_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(labels_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Labels saved to: {LABELS_SAVE_PATH}\")\n",
    "print(f\"\\nLabel mapping:\")\n",
    "for idx, label in labels_dict.items():\n",
    "    print(f\"  {idx}: {label}\")\n",
    "\n",
    "# Print as Python list for copy-paste\n",
    "print(f\"\\nğŸ“‹ Copy this for your prediction code:\")\n",
    "print(f\"ETH_LANDMARK_LABELS = {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 13: TEST PREDICTION ============\n",
    "def predict_from_image(image_path, model, hands, classes):\n",
    "    \"\"\"Test prediction on a single image\"\"\"\n",
    "    landmarks = extract_hand_landmarks(image_path, hands)\n",
    "    \n",
    "    if landmarks is None:\n",
    "        return \"No hand detected\", 0\n",
    "    \n",
    "    # Predict\n",
    "    landmarks = landmarks.reshape(1, -1)\n",
    "    prediction = model.predict(landmarks, verbose=0)\n",
    "    \n",
    "    pred_idx = np.argmax(prediction[0])\n",
    "    confidence = prediction[0][pred_idx] * 100\n",
    "    \n",
    "    return classes[pred_idx], confidence\n",
    "\n",
    "# Test on random images\n",
    "print(\"Testing predictions on sample images:\\n\")\n",
    "\n",
    "with mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5) as hands:\n",
    "    for class_name in classes[:5]:\n",
    "        class_path = os.path.join(DATA_DIR, class_name)\n",
    "        images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        if images:\n",
    "            img_path = os.path.join(class_path, images[-1])  # Use last image (not in training)\n",
    "            pred_label, conf = predict_from_image(img_path, best_model, hands, classes)\n",
    "            status = \"âœ…\" if pred_label == class_name else \"âŒ\"\n",
    "            print(f\"{status} True: {class_name} | Predicted: {pred_label} ({conf:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 14: SUMMARY & DOWNLOAD ============\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ“ Model saved to: {MODEL_SAVE_PATH}\")\n",
    "print(f\"ğŸ“ Labels saved to: {LABELS_SAVE_PATH}\")\n",
    "print(f\"\\nğŸ¯ Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"ğŸ“Š Number of classes: {num_classes}\")\n",
    "print(f\"ğŸ“Š Model size: ~{model.count_params() * 4 / 1024:.1f} KB\")\n",
    "print(f\"\\nâš¡ This model uses hand landmarks (63 features)\")\n",
    "print(f\"   instead of images (224x224x3 = 150,528 pixels)\")\n",
    "print(f\"   = 2,389x smaller input!\")\n",
    "print(f\"\\nğŸ“¥ Download these files and copy to your project:\")\n",
    "print(f\"   1. {MODEL_SAVE_PATH}\")\n",
    "print(f\"   2. {LABELS_SAVE_PATH}\")\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download(MODEL_SAVE_PATH)\n",
    "files.download(LABELS_SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
